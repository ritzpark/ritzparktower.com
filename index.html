<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Smart Motion & Human Detection</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no" />
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.13.0"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
  <style>
    body {
      background: #111;
      color: white;
      font-family: sans-serif;
      text-align: center;
      margin: 0;
      padding: 10px;
      overflow-x: hidden;
    }

    .video-container {
      display: flex;
      flex-direction: column;
      align-items: center;
      gap: 10px;
      margin-top: 10px;
    }

    video, canvas {
      width: 360px;
      height: 640px;
      background: black;
      border: 2px solid #333;
      object-fit: cover;
      transform: rotate(0deg); /* stays horizontal on mobile */
    }

    #status {
      margin-top: 10px;
      font-size: 1.2em;
    }

    .tag {
      font-size: 1.1em;
      margin-top: 6px;
      color: yellow;
    }

    button {
      padding: 8px 16px;
      font-size: 1em;
      margin-top: 10px;
    }

    @media (orientation: portrait) {
      video, canvas {
        transform: rotate(0deg); /* You can also force -90deg if needed */
      }
    }
  </style>
</head>
<body>
  <h2>Motion + Human Detection</h2>
  <button onclick="switchCamera()">ðŸ”„ Switch Camera</button>

  <div class="video-container">
    <video id="video" autoplay muted playsinline></video>
    <canvas id="canvas" width="360" height="640"></canvas>
  </div>

  <div id="status">Initializing...</div>
  <div class="tag" id="tagLabel">â€”</div>

  <script>
    const video = document.getElementById("video");
    const canvas = document.getElementById("canvas");
    const ctx = canvas.getContext("2d");
    const statusText = document.getElementById("status");
    const tagLabel = document.getElementById("tagLabel");

    let model = null;
    let currentFacing = "environment";
    let lastGray = null;
    let motionCooldown = 0;
    let lastSnapTime = 0;
    let detectLoopRunning = false;
    let stream = null;

    const MOTION_THRESHOLD = 50;
    const MOTION_AREA_THRESHOLD = 2500;
    const COOLDOWN_FRAMES = 20;
    const SNAP_INTERVAL_MS = 15000;

    async function startCamera(facingMode = "environment") {
      if (stream) stream.getTracks().forEach(track => track.stop());

      try {
        const constraints = {
          video: {
            width: { ideal: 360 },
            height: { ideal: 640 },
            facingMode: { exact: facingMode }
          }
        };
        stream = await navigator.mediaDevices.getUserMedia(constraints);
      } catch (err) {
        console.warn("Fallback to default camera");
        stream = await navigator.mediaDevices.getUserMedia({
          video: { width: 480, height: 270 }
        });
      }

      video.srcObject = stream;
      return new Promise(resolve => {
        video.onloadedmetadata = () => {
          video.play();
          resolve();
        };
      });
    }

    function getGrayscale(frame) {
      const gray = new Uint8ClampedArray(frame.width * frame.height);
      const data = frame.data;
      for (let i = 0; i < data.length; i += 4) {
        gray[i / 4] = (data[i] + data[i + 1] + data[i + 2]) / 3;
      }
      return gray;
    }

    async function detectLoop() {
      if (!detectLoopRunning || !model) return;

      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
      const frame = ctx.getImageData(0, 0, canvas.width, canvas.height);
      const gray = getGrayscale(frame);

      let motionPixels = 0;
      if (lastGray) {
        for (let i = 0; i < gray.length; i++) {
          if (Math.abs(gray[i] - lastGray[i]) > MOTION_THRESHOLD) {
            motionPixels++;
          }
        }
      }

      let motionDetected = false;
      if (motionPixels > MOTION_AREA_THRESHOLD) {
        motionDetected = true;
        motionCooldown = COOLDOWN_FRAMES;
      } else if (motionCooldown > 0) {
        motionDetected = true;
        motionCooldown--;
      }

      if (motionDetected) {
        statusText.textContent = "ðŸ”” Motion detected!";
        statusText.style.color = "orange";
        await detectHumanAndSnap();
      } else {
        statusText.textContent = "No motion";
        statusText.style.color = "lightgreen";
        tagLabel.textContent = "â€”";
      }

      lastGray = gray;
      requestAnimationFrame(detectLoop);
    }

    async function detectHumanAndSnap() {
      const now = Date.now();
      const predictions = await model.detect(video);
      let humanDetected = false;

      for (const p of predictions) {
        if (p.class === "person" && p.score > 0.6) {
          humanDetected = true;
          tagLabel.textContent = "ðŸ‘¤ Human detected";

          ctx.strokeStyle = "yellow";
          ctx.lineWidth = 2;
          ctx.strokeRect(p.bbox[0], p.bbox[1], p.bbox[2], p.bbox[3]);

          if (now - lastSnapTime >= SNAP_INTERVAL_MS) {
            saveSnapshot("Human");
            lastSnapTime = now;
          }
          break;
        }
      }

      if (!humanDetected) {
        tagLabel.textContent = "â€”";
      }
    }

    function saveSnapshot(label) {
      const snapCanvas = document.createElement("canvas");
      snapCanvas.width = 640;
      snapCanvas.height = 360;
      const snapCtx = snapCanvas.getContext("2d");
      snapCtx.drawImage(video, 0, 0, snapCanvas.width, snapCanvas.height);
      const timestamp = new Date().toISOString().replace(/[:.]/g, "-");
      const link = document.createElement("a");
      link.download = `${timestamp}_${label}.jpg`;
      link.href = snapCanvas.toDataURL("image/jpeg");
      link.click();
    }

    async function switchCamera() {
      currentFacing = currentFacing === "user" ? "environment" : "user";
      detectLoopRunning = false;
      statusText.textContent = "Switching camera...";
      await startCamera(currentFacing);
      lastGray = null;
      detectLoopRunning = true;
      requestAnimationFrame(detectLoop);
    }

    async function init() {
      await startCamera(currentFacing);
      statusText.textContent = "Loading model...";
      model = await cocoSsd.load();
      statusText.textContent = "Ready";
      detectLoopRunning = true;
      requestAnimationFrame(detectLoop);
    }

    init();
  </script>
</body>
</html>
